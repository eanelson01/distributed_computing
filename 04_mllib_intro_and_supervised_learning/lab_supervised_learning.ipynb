{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### University of Virginia\n",
    "### DS 7200: Distributed Computing\n",
    "### Lab: Supervised Learning\n",
    "### Last Updated: August 20, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "This project has two parts:\n",
    "- Part I: Classification - build and apply a logistic regression model on the Wisconsin Breast Cancer dataset.\n",
    "- Part II: Regression - build and apply a linear regression model on the California Housing dataset.\n",
    "\n",
    "**Total Possible Points: 10**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part I: Classification (5 POINTS)\n",
    "\n",
    "Here are the specifications and grading breakdown:\n",
    "\n",
    "- the target variable is `diagnosis`\n",
    "- use `f1`, `f2` as predictors (1 PT)\n",
    "- split data into 60% training set, 40% test set \n",
    "- standardize the predictors (1 PT)\n",
    "- use seed=314 whenever a seed is needed\n",
    "- fit a Logistic Regression model with an intercept (1 PT)\n",
    "- compute and show the area under the ROC curve for the test set (2 PTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/09/27 14:14:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "DATA_FILEPATH = 'wisc_breast_cancer_w_fields.csv'\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Wisc BRCA\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enter code and solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# need to define a schema so that the data are not strings, I was getting an error with the vector assembler functions when strings\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField('id', StringType(), False), \n",
    "        StructField('diagnosis', StringType(), False)\n",
    "    ] + [StructField(f'f{x}', DoubleType(), False) for x in range(1, 31)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/09/27 14:14:54 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+-----+-----+-----+------+-------+-------+------+-------+------+-------+------+------+-----+-----+--------+-------+-------+-------+-------+--------+-----+-----+-----+------+------+------+------+------+------+-------+\n",
      "|    id|diagnosis|   f1|   f2|   f3|    f4|     f5|     f6|    f7|     f8|    f9|    f10|   f11|   f12|  f13|  f14|     f15|    f16|    f17|    f18|    f19|     f20|  f21|  f22|  f23|   f24|   f25|   f26|   f27|   f28|   f29|    f30|\n",
      "+------+---------+-----+-----+-----+------+-------+-------+------+-------+------+-------+------+------+-----+-----+--------+-------+-------+-------+-------+--------+-----+-----+-----+------+------+------+------+------+------+-------+\n",
      "|842302|        M|17.99|10.38|122.8|1001.0| 0.1184| 0.2776|0.3001| 0.1471|0.2419|0.07871| 1.095|0.9053|8.589|153.4|0.006399|0.04904|0.05373|0.01587|0.03003|0.006193|25.38|17.33|184.6|2019.0|0.1622|0.6656|0.7119|0.2654|0.4601| 0.1189|\n",
      "|842517|        M|20.57|17.77|132.9|1326.0|0.08474|0.07864|0.0869|0.07017|0.1812|0.05667|0.5435|0.7339|3.398|74.08|0.005225|0.01308| 0.0186| 0.0134|0.01389|0.003532|24.99|23.41|158.8|1956.0|0.1238|0.1866|0.2416| 0.186| 0.275|0.08902|\n",
      "+------+---------+-----+-----+-----+------+-------+-------+------+-------+------+-------+------+------+-----+-----+--------+-------+-------+-------+-------+--------+-----+-----+-----+------+------+------+------+------+------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read in the file\n",
    "\n",
    "df = spark.read.csv(DATA_FILEPATH, schema = schema, header = True)\n",
    "\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- diagnosis: string (nullable = true)\n",
      " |-- f1: double (nullable = true)\n",
      " |-- f2: double (nullable = true)\n",
      " |-- f3: double (nullable = true)\n",
      " |-- f4: double (nullable = true)\n",
      " |-- f5: double (nullable = true)\n",
      " |-- f6: double (nullable = true)\n",
      " |-- f7: double (nullable = true)\n",
      " |-- f8: double (nullable = true)\n",
      " |-- f9: double (nullable = true)\n",
      " |-- f10: double (nullable = true)\n",
      " |-- f11: double (nullable = true)\n",
      " |-- f12: double (nullable = true)\n",
      " |-- f13: double (nullable = true)\n",
      " |-- f14: double (nullable = true)\n",
      " |-- f15: double (nullable = true)\n",
      " |-- f16: double (nullable = true)\n",
      " |-- f17: double (nullable = true)\n",
      " |-- f18: double (nullable = true)\n",
      " |-- f19: double (nullable = true)\n",
      " |-- f20: double (nullable = true)\n",
      " |-- f21: double (nullable = true)\n",
      " |-- f22: double (nullable = true)\n",
      " |-- f23: double (nullable = true)\n",
      " |-- f24: double (nullable = true)\n",
      " |-- f25: double (nullable = true)\n",
      " |-- f26: double (nullable = true)\n",
      " |-- f27: double (nullable = true)\n",
      " |-- f28: double (nullable = true)\n",
      " |-- f29: double (nullable = true)\n",
      " |-- f30: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show the schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# I need to look at the diagnosis column and turn to a 0 or 1 classification for logistic regression\n",
    "# using string indexer to do this, I found this function here:\n",
    "# https://datascience.stackexchange.com/questions/6268/how-to-convert-categorical-data-to-numerical-data-in-pyspark\n",
    "\n",
    "indexer = StringIndexer(inputCol = 'diagnosis', outputCol = 'diagnosisIndex')\n",
    "df = indexer.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|diagnosis|count|\n",
      "+---------+-----+\n",
      "|        B|  357|\n",
      "|        M|  212|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# string indexer give 0 to \"the most frequent label gets index 0\"\n",
    "# in this case B is 0 and M is 1\n",
    "df.groupBy('diagnosis').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+\n",
      "|diagnosis|diagnosisIndex|\n",
      "+---------+--------------+\n",
      "|        M|           1.0|\n",
      "|        M|           1.0|\n",
      "|        M|           1.0|\n",
      "|        M|           1.0|\n",
      "|        M|           1.0|\n",
      "|        M|           1.0|\n",
      "|        M|           1.0|\n",
      "|        M|           1.0|\n",
      "|        M|           1.0|\n",
      "|        M|           1.0|\n",
      "|        M|           1.0|\n",
      "|        M|           1.0|\n",
      "|        M|           1.0|\n",
      "|        M|           1.0|\n",
      "|        M|           1.0|\n",
      "|        M|           1.0|\n",
      "|        M|           1.0|\n",
      "|        M|           1.0|\n",
      "|        M|           1.0|\n",
      "|        B|           0.0|\n",
      "+---------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# confirm again by showing columns side by side\n",
    "df.select(['diagnosis', 'diagnosisIndex']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use f1, f2 as predictors (1 PT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+-----+-------------+\n",
      "|diagnosisIndex|   f1|   f2|     features|\n",
      "+--------------+-----+-----+-------------+\n",
      "|           1.0|17.99|10.38|[17.99,10.38]|\n",
      "|           1.0|20.57|17.77|[20.57,17.77]|\n",
      "+--------------+-----+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select the target and predictor variables\n",
    "assembler = VectorAssembler(\n",
    "    inputCols = ['f1', 'f2'],\n",
    "    outputCol = 'features'\n",
    ")\n",
    "\n",
    "# use the assembler\n",
    "tr = assembler.transform(df.select(['diagnosisIndex', 'f1', 'f2']))\n",
    "tr.select(\"*\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split data into 60% training set, 40% test set\n",
    "splits = tr.randomSplit([0.6, 0.4], seed = 314)\n",
    "train = splits[0]\n",
    "test = splits[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### standardize the predictors (1 PT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# standardize the predictor values\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features\", \n",
    "    outputCol=\"scaledFeatures\", \n",
    "    withMean = True\n",
    ")\n",
    "\n",
    "# scale the training\n",
    "train_scalerModel = scaler.fit(train)\n",
    "train_scaled = train_scalerModel.transform(train)\n",
    "\n",
    "# scale the testing\n",
    "test_scalerModel = scaler.fit(test)\n",
    "test_scaled = test_scalerModel.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fit a Logistic Regression model with an intercept (1 PT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fit the logisitc regression\n",
    "lr = LogisticRegression(\n",
    "    labelCol='diagnosisIndex',\n",
    "    featuresCol='scaledFeatures',\n",
    "    maxIter=10, \n",
    "    regParam=0.3, \n",
    "    elasticNetParam=0.8\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+----------+\n",
      "|probability                             |prediction|\n",
      "+----------------------------------------+----------+\n",
      "|[0.775085854099279,0.224914145900721]   |0.0       |\n",
      "|[0.766078015770669,0.23392198422933097] |0.0       |\n",
      "|[0.7584534742379524,0.2415465257620476] |0.0       |\n",
      "|[0.758046455448967,0.24195354455103302] |0.0       |\n",
      "|[0.7558404948265022,0.24415950517349783]|0.0       |\n",
      "+----------------------------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the predictions from the the model on the test data\n",
    "lrPred = lrModel.transform(test_scaled)\n",
    "lrPred.select('probability', 'prediction').show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+-----+------------+--------------------+--------------------+--------------------+----------+\n",
      "|diagnosisIndex|   f1|   f2|    features|      scaledFeatures|       rawPrediction|         probability|prediction|\n",
      "+--------------+-----+-----+------------+--------------------+--------------------+--------------------+----------+\n",
      "|           0.0| 7.76|24.54|[7.76,24.54]|[-1.8424000827161...|[1.23725504708864...|[0.77508585409927...|       0.0|\n",
      "|           0.0|8.219| 20.7|[8.219,20.7]|[-1.7088204423750...|[1.18629635378102...|[0.76607801577066...|       0.0|\n",
      "+--------------+-----+-----+------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lrPred.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compute and show the area under the ROC curve for the test set (2 PTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area Under PR Curve: 0.922694056677676\n"
     ]
    }
   ],
   "source": [
    "# set up evaluator\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    labelCol=\"diagnosisIndex\",\n",
    "    metricName=\"areaUnderPR\"\n",
    ")\n",
    "\n",
    "aupr = evaluator.evaluate(lrPred)\n",
    "\n",
    "# show the area under the curve\n",
    "print('Area Under PR Curve:', aupr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is junk code on the start of trying to create the ROC curve but do not think we need it anymore\n",
    "\n",
    "# # turn the data into an Rdd for the probabilities\n",
    "# test_probs_and_labs = lrPred.select('probability', 'diagnosisIndex').rdd \\\n",
    "#                             .map(lambda x: (x.probability[1], x.diagnosisIndex))\n",
    "\n",
    "# # get the curve\n",
    "# test_probs_and_labs.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part II: Regression (5 POINTS)\n",
    "\n",
    "In this project, you will work with the California Home Price dataset to train a regression model and predict median home prices. Here are the specifications and grading breakdown:\n",
    "\n",
    "- Scale the response variable median_house_value, dividing by 100000 (1 PT)\n",
    "\n",
    "- Split data into train set (80%), test set (20%) using seed=314 (1 PT)\n",
    "\n",
    "- Add new predictor: `rooms_per_household`\n",
    "\n",
    "- In the training set, select all of these features and standardize them: (1 PT)\n",
    "\n",
    "feats = [\"total_bedrooms\", \n",
    "         \"population\", \n",
    "         \"households\", \n",
    "         \"median_income\", \n",
    "         \"rooms_per_household\"]\n",
    "\n",
    "- Fit a linear regression model on the training set with these parameters:\n",
    "\n",
    "  - maxIter=10\n",
    "  - regParam=0.3\n",
    "  - elasticNetParam=0.8  \n",
    "\n",
    "\n",
    "- Compute the MSE on the test set (2 PTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILEPATH2 = 'cal_housing_data_preproc_w_header.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the schema\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField('median_house_value', DoubleType(), False), \n",
    "        StructField('median_income', DoubleType(), False),\n",
    "        StructField('housing_median_age', DoubleType(), False),\n",
    "        StructField('total_rooms', DoubleType(), False),\n",
    "        StructField('total_bedrooms', DoubleType(), False),\n",
    "        StructField('population', DoubleType(), False),\n",
    "        StructField('households', DoubleType(), False),\n",
    "        StructField('latitude', DoubleType(), False),\n",
    "        StructField('longitude', DoubleType(), False)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# read in the data frame\n",
    "house_data = spark.read.csv(DATA_FILEPATH2, schema = schema, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+\n",
      "|median_house_value|median_income|housing_median_age|total_rooms|total_bedrooms|population|households|latitude|longitude|\n",
      "+------------------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+\n",
      "|          452600.0|       8.3252|              41.0|      880.0|         129.0|     322.0|     126.0|   37.88|  -122.23|\n",
      "|          358500.0|       8.3014|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|   37.86|  -122.22|\n",
      "+------------------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "house_data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- median_house_value: double (nullable = true)\n",
      " |-- median_income: double (nullable = true)\n",
      " |-- housing_median_age: double (nullable = true)\n",
      " |-- total_rooms: double (nullable = true)\n",
      " |-- total_bedrooms: double (nullable = true)\n",
      " |-- population: double (nullable = true)\n",
      " |-- households: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "house_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enter code and solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale the response variable median_house_value, dividing by 100000 (1 PT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+\n",
      "|median_house_value|median_income|housing_median_age|total_rooms|total_bedrooms|population|households|latitude|longitude|\n",
      "+------------------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+\n",
      "|             4.526|       8.3252|              41.0|      880.0|         129.0|     322.0|     126.0|   37.88|  -122.23|\n",
      "|             3.585|       8.3014|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|   37.86|  -122.22|\n",
      "+------------------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Scale the response variable median_house_value, dividing by 100000\n",
    "\n",
    "house_data = house_data.withColumn('median_house_value', house_data.median_house_value / 100000)\n",
    "house_data.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data into train set (80%), test set (20%) using seed=314 (1 PT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train # 16472\n",
      "Test # 4168\n"
     ]
    }
   ],
   "source": [
    "# Split data into train set (80%), test set (20%) using seed=314\n",
    "\n",
    "house_split = house_data.randomSplit([0.8, 0.2], seed = 314)\n",
    "train = house_split[0]\n",
    "test = house_split[1]\n",
    "\n",
    "print('Train #', train.count())\n",
    "print('Test #', test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+-------------------+\n",
      "|median_house_value|median_income|housing_median_age|total_rooms|total_bedrooms|population|households|latitude|longitude|rooms_per_household|\n",
      "+------------------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+-------------------+\n",
      "|           0.14999|        0.536|              36.0|       98.0|          28.0|      18.0|       8.0|   40.31|  -123.17|              12.25|\n",
      "|           0.14999|       1.6607|              16.0|      255.0|          73.0|      85.0|      38.0|   39.71|  -122.74| 6.7105263157894735|\n",
      "+------------------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add new predictor: rooms_per_household\n",
    "train = train.withColumn('rooms_per_household', train.total_rooms / train.households)\n",
    "test = test.withColumn('rooms_per_household', test.total_rooms / test.households)\n",
    "\n",
    "train.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the training set, select all of these features and standardize them: (1 PT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+-------------------+--------------------+--------------------+\n",
      "|median_house_value|median_income|housing_median_age|total_rooms|total_bedrooms|population|households|latitude|longitude|rooms_per_household|            features|      scaledFeatures|\n",
      "+------------------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+-------------------+--------------------+--------------------+\n",
      "|           0.14999|        0.536|              36.0|       98.0|          28.0|      18.0|       8.0|   40.31|  -123.17|              12.25|[28.0,18.0,8.0,0....|[-1.2194405023784...|\n",
      "|           0.14999|       1.6607|              16.0|      255.0|          73.0|      85.0|      38.0|   39.71|  -122.74| 6.7105263157894735|[73.0,85.0,38.0,1...|[-1.1120135412815...|\n",
      "+------------------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+-------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In the training set, select all of these features and standardize them\n",
    "# feats = [\"total_bedrooms\", \"population\", \"households\", \"median_income\", \"rooms_per_household\"]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols = [\"total_bedrooms\", \"population\", \"households\", \"median_income\", \"rooms_per_household\"],\n",
    "    outputCol = 'features'\n",
    ")\n",
    "\n",
    "# use the assembler\n",
    "train = assembler.transform(train)\n",
    "test = assembler.transform(test)\n",
    "\n",
    "# now create the scalar\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features\", \n",
    "    outputCol=\"scaledFeatures\", \n",
    "    withMean = True\n",
    ")\n",
    "\n",
    "# scale the training\n",
    "train_scalerModel = scaler.fit(train)\n",
    "train_scaled = train_scalerModel.transform(train)\n",
    "\n",
    "train_scaled.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fit a linear regression model on the training set with these parameters:\n",
    "# maxIter = 10\n",
    "# regParam = 0.3\n",
    "# elasticNetParam = 0.8\n",
    "\n",
    "lr = LinearRegression(featuresCol='scaledFeatures',         # feature vector name\n",
    "                      labelCol='median_house_value',  # target variable name\n",
    "                      maxIter=10,\n",
    "                      regParam=0.3, \n",
    "                      elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the MSE on the test set (2 PTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+------------------+-----------+--------------+----------+----------+--------+---------+-------------------+--------------------+--------------------+------------------+\n",
      "|median_house_value|     median_income|housing_median_age|total_rooms|total_bedrooms|population|households|latitude|longitude|rooms_per_household|            features|      scaledFeatures|        prediction|\n",
      "+------------------+------------------+------------------+-----------+--------------+----------+----------+--------+---------+-------------------+--------------------+--------------------+------------------+\n",
      "|           0.14999|            4.1932|              52.0|      803.0|         267.0|     628.0|     225.0|   34.24|  -117.86|  3.568888888888889|[267.0,628.0,225....|[-0.6209227643583...| 2.160114332761376|\n",
      "|             0.225|0.7916999999999998|              52.0|      107.0|          79.0|     167.0|      53.0|   37.95|  -121.29|  2.018867924528302|[79.0,167.0,53.0,...|[-1.0576496037251...|1.2434000052168512|\n",
      "+------------------+------------------+------------------+-----------+--------------+----------+----------+--------+---------+-------------------+--------------------+--------------------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Compute the MSE on the test set\n",
    "# have to scale the test\n",
    "\n",
    "test_scaler = scaler.fit(test)\n",
    "test_scaled = test_scaler.transform(test)\n",
    "\n",
    "lrPred = lrModel.transform(test_scaled)\n",
    "lrPred.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error 0.7630394708043784\n"
     ]
    }
   ],
   "source": [
    "# actually getting the MSE\n",
    "ev = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"median_house_value\")\n",
    "\n",
    "# print the MSE\n",
    "print('Mean Squared Error', ev.evaluate(lrPred, {ev.metricName: 'mse'}))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "DS7200 Spark 3.3",
   "language": "python",
   "name": "ds5110_spark3.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
